{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "# MAKE SURE TO SHUFFLE IMPORT ORDER AND DELETE USELESS IMPORTS\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, Dropout, BatchNorm2d, BCELoss\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Directories\n",
    "maskdir = \"F:\\Libraries\\Documents\\ALL\\School\\GrDip\\comp6721_applied_artificial_intelligence\\project\\FaceMaskCNN\\data\\masknomask\\dataset\\dataset\\with_mask\"\n",
    "nomaskdir = \"F:\\Libraries\\Documents\\ALL\\School\\GrDip\\comp6721_applied_artificial_intelligence\\project\\FaceMaskCNN\\data\\masknomask\\dataset\\dataset\\without_mask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_images(im_dir, prefix=\"\", suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Utility function that renames all the images in a folder to have increasing integer numbers.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.isdir(im_dir):\n",
    "        print(\"The specified directory '{}' does not exist.\")\n",
    "        return False\n",
    "    \n",
    "    for i, fname in enumerate(os.listdir(im_dir)):\n",
    "        _, ext = os.path.splitext(fname)\n",
    "        new_name = prefix + str(i) + suffix + ext\n",
    "        \n",
    "        try:\n",
    "            os.rename(os.path.join(im_dir,fname), os.path.join(im_dir,new_name))\n",
    "        except FileExistsError:\n",
    "            continue\n",
    "            \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - rename images in mask set\n",
    "rename_images(\"F:\\Libraries\\Documents\\ALL\\School\\GrDip\\comp6721_applied_artificial_intelligence\\project\\FaceMaskCNN\\data\\masknomask\\dataset\\dataset\\with_mask\",\n",
    "             suffix=\"_m\")\n",
    "\n",
    "# - rename images in no mask set\n",
    "rename_images(\"F:\\Libraries\\Documents\\ALL\\School\\GrDip\\comp6721_applied_artificial_intelligence\\project\\FaceMaskCNN\\data\\masknomask\\dataset\\dataset\\without_mask\",\n",
    "             suffix=\"_nm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Creation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transforms\n",
    "preprocess = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize([32,32])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Import Data #\n",
    "#\n",
    "\n",
    "# Create the labeled data indices\n",
    "df1 = pd.DataFrame(os.listdir(maskdir)).rename({0:\"file_id\"}, axis=1)\n",
    "df1[\"label_literal\"] = os.path.split(maskdir)[1]\n",
    "\n",
    "df2 = pd.DataFrame(os.listdir(nomaskdir)).rename({0:\"file_id\"}, axis=1)\n",
    "df2[\"label_literal\"] = os.path.split(nomaskdir)[1]\n",
    "\n",
    "\n",
    "data = pd.concat([df1, df2])\n",
    "data = data.reset_index(drop=True)\n",
    "data[\"label\"] = data[\"label_literal\"].apply(lambda x : 0 if x==\"without_mask\" else 1)\n",
    "\n",
    "# Load the images\n",
    "labels = data[\"label\"].to_numpy()\n",
    "images = []\n",
    "for row in data.itertuples():\n",
    "    \n",
    "    if row.label_literal == \"with_mask\":\n",
    "        im_path = os.path.join(maskdir, row.file_id)\n",
    "    elif row.label_literal == \"without_mask\":\n",
    "        im_path = os.path.join(nomaskdir, row.file_id)\n",
    "        \n",
    "    image = imread(im_path, as_gray=True)\n",
    "    \n",
    "    # Normalize image so that it is between 0 and 1\n",
    "    image = image/255.0\n",
    "    \n",
    "    # Change data to float32 (dont think this is necessary? Might help speed up processing)\n",
    "    image = image.astype(\"float32\")\n",
    "    \n",
    "    # Apply Torch transforms\n",
    "    image = preprocess(image)\n",
    "    \n",
    "    images.append(image)\n",
    "\n",
    "    \n",
    "#images = np.array(images)\n",
    "image_tensor = torch.cat(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2011526eb88>"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_tensor[121])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "labels = torch.tensor(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_tensor, labels, test_size=0.3, stratify=labels, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the channel dimension to the train and test sets\n",
    "X_train = torch.unsqueeze(X_train, 1)\n",
    "X_test = torch.unsqueeze(X_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2682, 1, 32, 32])\n",
      "torch.Size([1150, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "\n",
    "# class MaskNoMaskDset(Dataset):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CNN\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            Dropout(0.5),\n",
    "            \n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(4 * 8 * 8, 2)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 4, 32, 32]              40\n",
      "       BatchNorm2d-2            [-1, 4, 32, 32]               8\n",
      "              ReLU-3            [-1, 4, 32, 32]               0\n",
      "         MaxPool2d-4            [-1, 4, 16, 16]               0\n",
      "           Dropout-5            [-1, 4, 16, 16]               0\n",
      "            Conv2d-6            [-1, 4, 16, 16]             148\n",
      "       BatchNorm2d-7            [-1, 4, 16, 16]               8\n",
      "              ReLU-8            [-1, 4, 16, 16]               0\n",
      "         MaxPool2d-9              [-1, 4, 8, 8]               0\n",
      "           Linear-10                    [-1, 2]             514\n",
      "================================================================\n",
      "Total params: 718\n",
      "Trainable params: 718\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.13\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "devnet = Net()\n",
    "summary(devnet, (1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining optimizers and loss function\n",
    "\n",
    "testmodel = Net()\n",
    "adam_opt = Adam(testmodel.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6557, -0.2302],\n",
       "        [-0.1317,  1.5396],\n",
       "        [-1.1518,  2.7985],\n",
       "        ...,\n",
       "        [-0.5567,  1.2203],\n",
       "        [ 0.8210, -0.5283],\n",
       "        [ 1.4116,  0.2917]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0      Loss: 0.719201385974884\n",
      "Epoch: 2      Loss: 0.7096483707427979\n",
      "Epoch: 4      Loss: 0.7002910375595093\n",
      "Epoch: 6      Loss: 0.6890263557434082\n",
      "Epoch: 8      Loss: 0.6776925921440125\n",
      "Epoch: 10      Loss: 0.6627748608589172\n",
      "Epoch: 12      Loss: 0.6536746621131897\n",
      "Epoch: 14      Loss: 0.6398228406906128\n",
      "Epoch: 16      Loss: 0.634195864200592\n",
      "Epoch: 18      Loss: 0.6208580732345581\n",
      "Epoch: 20      Loss: 0.6182875633239746\n",
      "Epoch: 22      Loss: 0.6018685698509216\n",
      "Epoch: 24      Loss: 0.5948046445846558\n",
      "Epoch: 26      Loss: 0.5804978013038635\n",
      "Epoch: 28      Loss: 0.5682235956192017\n",
      "Epoch: 30      Loss: 0.5604888796806335\n",
      "Epoch: 32      Loss: 0.5509119033813477\n",
      "Epoch: 34      Loss: 0.5408916473388672\n",
      "Epoch: 36      Loss: 0.5340945720672607\n",
      "Epoch: 38      Loss: 0.5213881134986877\n",
      "Epoch: 40      Loss: 0.5196241140365601\n",
      "Epoch: 42      Loss: 0.5116056203842163\n",
      "Epoch: 44      Loss: 0.5114365220069885\n",
      "Epoch: 46      Loss: 0.5050814747810364\n",
      "Epoch: 48      Loss: 0.4888530969619751\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "\n",
    "# torch.set_grad_enabled(True)\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "tr_loss = 0\n",
    "testmodel.train()\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    \n",
    "    adam_opt.zero_grad()\n",
    "    \n",
    "    output_train = testmodel(X_train)\n",
    "    \n",
    "    loss_train = criterion(output_train, y_train)\n",
    "    train_losses.append(loss_train)\n",
    "    \n",
    "    loss_train.backward()\n",
    "    adam_opt.step()\n",
    "    \n",
    "    if epoch%2 == 0:\n",
    "        print(\"Epoch: {}      Loss: {}\".format(epoch, loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_out = testmodel(X_test)\n",
    "\n",
    "softmax = torch.exp(test_out)\n",
    "prob = list(softmax.numpy())\n",
    "preds = np.argmax(prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7669565217391304"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcnnvenv",
   "language": "python",
   "name": "fcnnvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
